{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexaK88/Q_jpeg_pennylane/blob/main/full_flow_different_qubits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nrVJJ9YHoiPh",
        "outputId": "acaa8b6f-6a56-430c-f81d-fe144ccc4660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.44.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.6.1)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray==0.8.2 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.8.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (6.2.6)\n",
            "Requirement already satisfied: pennylane-lightning>=0.44 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.44.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning>=0.44->pennylane) (0.3.31.22.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.7.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2026.1.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.46.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.44.0)\n",
            "Requirement already satisfied: pennylane-lightning[gpu] in /usr/local/lib/python3.12/dist-packages (0.44.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.6.1)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray==0.8.2 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.8.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (6.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning[gpu]) (0.3.31.22.1)\n",
            "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning[gpu]) (0.44.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.7.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.3.0)\n",
            "Requirement already satisfied: custatevec-cu12 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (1.12.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.6.77)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2026.1.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.46.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane\n",
        "!pip install pennylane pennylane-lightning[gpu]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KROFpzJsoxfN"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2-JpS4QpoxBj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pennylane as qml\n",
        "from pennylane.templates import QFT\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import fetch_openml, load_digits\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from pennylane import numpy as pnp\n",
        "from skimage.transform import resize\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPeJfiDJo-qn"
      },
      "source": [
        "### Step 1:  Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load the MNIST dataset from openML.\n",
        "- X is the pixel data\n",
        "- y is the labels\n",
        "- converting everything to `uint8` here to ensure all values are integers in [0, 255]"
      ],
      "metadata": {
        "id": "5JJh_V_1hfrh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "amm-B8_Xdzyf"
      },
      "outputs": [],
      "source": [
        "# loading mnist from openML\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "X = mnist['data'].astype(np.uint8) # better to convert for binerization\n",
        "y = mnist['target'].astype(np.uint8)\n",
        "y = y.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeMOrBB2ognh"
      },
      "source": [
        "Next, we focus on 2 classes, i.e. binary classification.\n",
        "Here, I've been experimenting with different classes, and I stopped on 4 vs 9, cause they have more subtle difference in pixels, they are similar looking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNShS1qTpNI7",
        "outputId": "e776c38c-1062-4fc7-83d8-14cc5d44e7d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13782, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# focus on binary classification\n",
        "mask = (y == 4) | (y == 9)\n",
        "X, y = X[mask], y[mask]\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I take only the first `n_samples`.\n",
        "- I convert X to a NumPy array, and shuffling the data randomly"
      ],
      "metadata": {
        "id": "6qdDAqkBizaZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "GdbQMsOdpoWz"
      },
      "outputs": [],
      "source": [
        "n_samples = 100 # restricting to 6000 samples for now\n",
        "\n",
        "X = X.values if hasattr(X, \"values\") else X # safer conversion\n",
        "perm = np.random.permutation(len(X))\n",
        "X, y = X[perm], y[perm]\n",
        "\n",
        "X = X[:n_samples]\n",
        "y = y[:n_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFV8_UnGptzJ"
      },
      "source": [
        "Now, I normalise pixel intensities.\n",
        "- [0, 255] -> [0, 1]\n",
        "- reshaping images back to 2D for resizing, i.e to 28x28 array with float values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69wNTvdppCb",
        "outputId": "d3682aab-e17d-4dcf-99cf-bbf148887acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 28, 28)\n",
            "Pixel range: 0.0 1.0\n"
          ]
        }
      ],
      "source": [
        "X = X / 255.0\n",
        "X = X.reshape(-1, 28, 28)\n",
        "\n",
        "print(X.shape)\n",
        "print(\"Pixel range:\", X.min(), X.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6fKM38QpyO8"
      },
      "source": [
        "And now I reduce images to 8x8 + flattening to (, 64)\n",
        "- resize -> flatten -> normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXfPx1nwpq31",
        "outputId": "b3930e78-a416-4cf3-f539-248861cb4e41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# convert each 28x28 binarised image to 8x8, then flatten to length 64\n",
        "def to_8x8_vector(img_row):\n",
        "    img_8x8 = resize(\n",
        "        img_row,\n",
        "        (8, 8),\n",
        "        anti_aliasing=False,\n",
        "        preserve_range=True,\n",
        "        order=1 # controlling interpolation\n",
        "    )\n",
        "    img_8x8 = img_8x8.flatten()\n",
        "    s = np.sum(img_8x8)\n",
        "\n",
        "    if s > 0:\n",
        "        img_8x8 = np.sqrt(img_8x8 / s)\n",
        "    else:\n",
        "        img_8x8 = np.zeros_like(img_8x8)\n",
        "        img_8x8[0] = 1.0\n",
        "      # should be shape (64,)\n",
        "    return img_8x8\n",
        "\n",
        "# apply transformation to all images\n",
        "X_8x8 = np.array([to_8x8_vector(x) for x in X], dtype=float)\n",
        "X_8x8.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycrMsoOtp2BY",
        "outputId": "d6593161-e35f-46a1-fd3a-f5103431490d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any NaNs? False\n",
            "Norm check: 0.9999999999999999 1.0\n"
          ]
        }
      ],
      "source": [
        "# sanity check, make sure no NaNs exist and all vectors are normalised, i.e. norm is around 1\n",
        "print(\"Any NaNs?\", np.isnan(X_8x8).any())\n",
        "print(\"Norm check:\", np.min(np.linalg.norm(X_8x8, axis=1)), np.max(np.linalg.norm(X_8x8, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUuSiBQfp_Sf"
      },
      "source": [
        "I'm gonna do the splitting here, and carry both representations consistently.\n",
        "- qek inputs: (64,) flattened and normalized vectors, for quantum kernel embedding\n",
        "- qjpeg: 28x28 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbFUj8-Zp9fc",
        "outputId": "0735cede-668c-422b-c8f0-c38e7de74d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QEK train/test: (80, 64) (20, 64)\n",
            "IMG train/test: (80, 28, 28) (20, 28, 28)\n",
            "Labels train/test: (80,) (20,)\n"
          ]
        }
      ],
      "source": [
        "idx = np.arange(n_samples)\n",
        "\n",
        "idx_train, idx_test, y_train, y_test = train_test_split(\n",
        "    idx, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# QEK inputs (8x8 -> 64 -> normed)\n",
        "X_train_qek = X_8x8[idx_train]\n",
        "X_test_qek  = X_8x8[idx_test]\n",
        "\n",
        "# QJPEG inputs (28x28 binary images)\n",
        "X_train_img = X[idx_train]\n",
        "X_test_img  = X[idx_test]\n",
        "\n",
        "print(\"QEK train/test:\", X_train_qek.shape, X_test_qek.shape)\n",
        "print(\"IMG train/test:\", X_train_img.shape, X_test_img.shape)\n",
        "print(\"Labels train/test:\", y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQUmF-0YqOjS"
      },
      "source": [
        "Data preparation is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxklKVp_qRRT"
      },
      "source": [
        "### Step 2: Quantum Embedding & Kernel Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define number of qubits and device."
      ],
      "metadata": {
        "id": "DiuzYJktY-lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"lightning.qubit\"\n",
        "n_qubits = 6\n",
        "n_layers = 2          # number of trainable layers\n",
        "batch_size = 8       # bigger batch for stability\n",
        "n_steps = 50          # training steps\n",
        "stepsize = 2e-4       # smaller learning rate\n",
        "eps = 1e-8      # small epsilon to prevent division by zero\n",
        "wires = range(n_qubits)\n",
        "qubit_list = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "g1l-xD10kYsG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NDnTjviKqYmk"
      },
      "outputs": [],
      "source": [
        "dev = qml.device(device, wires=n_qubits, shots=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_map(x, theta, wires):\n",
        "    n_qubits = len(wires)\n",
        "    qml.AmplitudeEmbedding(x, wires=wires, pad_with=0.0, normalize=True)\n",
        "    for l in range(theta.shape[0]):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RX(theta[l, i, 0], wires=wires[i])\n",
        "            qml.RY(theta[l, i, 1], wires=wires[i])\n",
        "            qml.RZ(theta[l, i, 2], wires=wires[i])\n",
        "        for i in range(n_qubits - 1):\n",
        "            qml.CNOT(wires=[wires[i], wires[i+1]])\n",
        "        qml.CNOT(wires=[wires[-1], wires[0]])\n",
        "\n",
        "\n",
        "def train_quantum_kernel(X, y, n_qubits,\n",
        "                         n_layers=2,\n",
        "                         n_steps=50,\n",
        "                         batch_size=8,\n",
        "                         stepsize=2e-4,\n",
        "                         ema_alpha=0.9,\n",
        "                         verbose=True\n",
        "                         ):\n",
        "    wires = range(n_qubits)\n",
        "    dev = qml.device(\"lightning.qubit\", wires=n_qubits, shots=None)\n",
        "\n",
        "    @qml.qnode(dev, interface=\"autograd\")\n",
        "    def kernel_qnode(x1, x2, theta):\n",
        "        feature_map(x1, theta, wires)\n",
        "        qml.adjoint(feature_map)(x2, theta, wires)\n",
        "        return qml.expval(qml.Projector([0]*n_qubits, wires=wires))\n",
        "\n",
        "    def kernel_matrix(X1, X2, theta):\n",
        "        return qml.math.stack([\n",
        "            qml.math.stack([\n",
        "                kernel_qnode(x1, x2, theta)\n",
        "                for x2 in X2\n",
        "            ])\n",
        "            for x1 in X1\n",
        "        ])\n",
        "\n",
        "    def center_kernel(K):\n",
        "        n = K.shape[0]\n",
        "        H = qml.numpy.eye(n) - qml.numpy.ones((n, n)) / n\n",
        "        return H @ K @ H\n",
        "\n",
        "    def kernel_alignment_loss(theta):\n",
        "        idx = np.random.choice(len(X), batch_size, replace=False)\n",
        "        Xb = X[idx]\n",
        "        yb = y[idx]\n",
        "\n",
        "        K = center_kernel(kernel_matrix(Xb, Xb, theta))\n",
        "        y_pm = (2 * yb - 1).astype(float)\n",
        "        yy = qml.math.outer(y_pm, y_pm)\n",
        "\n",
        "        K /= qml.math.linalg.norm(K)\n",
        "        yy /= qml.math.linalg.norm(yy)\n",
        "\n",
        "        return -qml.math.sum(K * yy)\n",
        "\n",
        "    theta = 0.01 * qml.numpy.random.randn(n_layers, n_qubits, 3)\n",
        "    opt = qml.AdamOptimizer(stepsize)\n",
        "\n",
        "    ema = None\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        theta, loss = opt.step_and_cost(kernel_alignment_loss, theta)\n",
        "\n",
        "        loss_val = float(loss)\n",
        "        ema = loss_val if ema is None else ema_alpha * ema + (1 - ema_alpha) * loss_val\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"Step {step:02d} | \"\n",
        "                f\"loss = {loss_val:.4f} | \"\n",
        "                f\"ema = {ema:.4f}\"\n",
        "            )\n",
        "\n",
        "    return theta, kernel_qnode\n"
      ],
      "metadata": {
        "id": "F8UrF4hN6kKT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_qek_inputs(X, n_qubits, eps=1e-12):\n",
        "    dim = 2 ** n_qubits\n",
        "    Xq = X[:, :dim].copy()\n",
        "\n",
        "    # Renormalize each sample\n",
        "    norms = np.linalg.norm(Xq, axis=1, keepdims=True)\n",
        "    # If a vector is all zeros, replace its norm with 1.0 to avoid division by zero\n",
        "    # and then set the vector to a valid normalized state (e.g., |0...0>)\n",
        "    zero_norm_indices = np.where(norms.flatten() < eps)[0]\n",
        "    if len(zero_norm_indices) > 0:\n",
        "        Xq[zero_norm_indices, 0] = 1.0  # Set first element to 1 for |0...0>\n",
        "        Xq[zero_norm_indices, 1:] = 0.0 # Set rest to 0\n",
        "        norms[zero_norm_indices] = 1.0 # Ensure norm is 1.0\n",
        "\n",
        "    # Ensure norms are at least eps to prevent division by near-zero values\n",
        "    norms = np.maximum(norms, eps)\n",
        "    Xq = Xq / norms\n",
        "\n",
        "    return Xq"
      ],
      "metadata": {
        "id": "T1OTVNr_-n7u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for nq in qubit_list:\n",
        "    print(f\"\\nTraining kernel with {nq} qubits\")\n",
        "    Xq = prepare_qek_inputs(X_train_qek, nq)\n",
        "    # Pass the prepared Xq to train_quantum_kernel, not the original X_train_qek\n",
        "    theta, kernel = train_quantum_kernel(Xq, y_train, nq)\n",
        "\n",
        "    # Initialize results as a dictionary first if it's meant to store non-numeric keys\n",
        "    # or ensure nq is used as an index if results is a list\n",
        "    if isinstance(results, list): # if results is still an empty list, convert to dict\n",
        "        if not results: # only if empty\n",
        "            results = {}\n",
        "\n",
        "    results[nq] = {\n",
        "        \"theta\": theta,\n",
        "        \"kernel\": kernel\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9c5TYPX6zlH",
        "outputId": "db9b069d-5b8f-49cb-ba90-b32f1684d276"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training kernel with 4 qubits\n",
            "Step 00 | loss = -0.0458 | ema = -0.0458\n",
            "Step 01 | loss = -0.0598 | ema = -0.0472\n",
            "Step 02 | loss = -0.0185 | ema = -0.0443\n",
            "Step 03 | loss = -0.0795 | ema = -0.0478\n",
            "Step 04 | loss = -0.0280 | ema = -0.0458\n",
            "Step 05 | loss = -0.0422 | ema = -0.0455\n",
            "Step 06 | loss = -0.1289 | ema = -0.0538\n",
            "Step 07 | loss = -0.0879 | ema = -0.0572\n",
            "Step 08 | loss = -0.0211 | ema = -0.0536\n",
            "Step 09 | loss = -0.0280 | ema = -0.0511\n",
            "Step 10 | loss = -0.0615 | ema = -0.0521\n",
            "Step 11 | loss = -0.0626 | ema = -0.0532\n",
            "Step 12 | loss = -0.0493 | ema = -0.0528\n",
            "Step 13 | loss = -0.1401 | ema = -0.0615\n",
            "Step 14 | loss = -0.0804 | ema = -0.0634\n",
            "Step 15 | loss = -0.0850 | ema = -0.0655\n",
            "Step 16 | loss = -0.0545 | ema = -0.0644\n",
            "Step 17 | loss = -0.0535 | ema = -0.0633\n",
            "Step 18 | loss = -0.1241 | ema = -0.0694\n",
            "Step 19 | loss = -0.0455 | ema = -0.0670\n",
            "Step 20 | loss = -0.0528 | ema = -0.0656\n",
            "Step 21 | loss = -0.1089 | ema = -0.0699\n",
            "Step 22 | loss = -0.0123 | ema = -0.0642\n",
            "Step 23 | loss = -0.0792 | ema = -0.0657\n",
            "Step 24 | loss = -0.1132 | ema = -0.0704\n",
            "Step 25 | loss = -0.0194 | ema = -0.0653\n",
            "Step 26 | loss = -0.0451 | ema = -0.0633\n",
            "Step 27 | loss = -0.0465 | ema = -0.0616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pennylane/numpy/tensor.py:156: RuntimeWarning: invalid value encountered in divide\n",
            "  res = super().__array_ufunc__(ufunc, method, *args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/autograd/numpy/numpy_vjps.py:124: RuntimeWarning: divide by zero encountered in divide\n",
            "  lambda ans, x, y: unbroadcast_f(x, lambda g: g / y),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 28 | loss = nan | ema = nan\n",
            "Step 29 | loss = nan | ema = nan\n",
            "Step 30 | loss = nan | ema = nan\n",
            "Step 31 | loss = nan | ema = nan\n",
            "Step 32 | loss = nan | ema = nan\n",
            "Step 33 | loss = nan | ema = nan\n",
            "Step 34 | loss = nan | ema = nan\n",
            "Step 35 | loss = nan | ema = nan\n",
            "Step 36 | loss = nan | ema = nan\n",
            "Step 37 | loss = nan | ema = nan\n",
            "Step 38 | loss = nan | ema = nan\n",
            "Step 39 | loss = nan | ema = nan\n",
            "Step 40 | loss = nan | ema = nan\n",
            "Step 41 | loss = nan | ema = nan\n",
            "Step 42 | loss = nan | ema = nan\n",
            "Step 43 | loss = nan | ema = nan\n",
            "Step 44 | loss = nan | ema = nan\n",
            "Step 45 | loss = nan | ema = nan\n",
            "Step 46 | loss = nan | ema = nan\n",
            "Step 47 | loss = nan | ema = nan\n",
            "Step 48 | loss = nan | ema = nan\n",
            "Step 49 | loss = nan | ema = nan\n",
            "\n",
            "Training kernel with 5 qubits\n",
            "Step 00 | loss = -0.0617 | ema = -0.0617\n",
            "Step 01 | loss = -0.0334 | ema = -0.0588\n",
            "Step 02 | loss = -0.0382 | ema = -0.0568\n",
            "Step 03 | loss = -0.0468 | ema = -0.0558\n",
            "Step 04 | loss = -0.0794 | ema = -0.0581\n",
            "Step 05 | loss = -0.0272 | ema = -0.0550\n",
            "Step 06 | loss = -0.0659 | ema = -0.0561\n",
            "Step 07 | loss = -0.0478 | ema = -0.0553\n",
            "Step 08 | loss = -0.1003 | ema = -0.0598\n",
            "Step 09 | loss = -0.0517 | ema = -0.0590\n",
            "Step 10 | loss = -0.0637 | ema = -0.0595\n",
            "Step 11 | loss = -0.0634 | ema = -0.0598\n",
            "Step 12 | loss = -0.0540 | ema = -0.0593\n",
            "Step 13 | loss = -0.0321 | ema = -0.0565\n",
            "Step 14 | loss = -0.0492 | ema = -0.0558\n",
            "Step 15 | loss = -0.0209 | ema = -0.0523\n",
            "Step 16 | loss = -0.0181 | ema = -0.0489\n",
            "Step 17 | loss = -0.0601 | ema = -0.0500\n",
            "Step 18 | loss = -0.0590 | ema = -0.0509\n",
            "Step 19 | loss = -0.0608 | ema = -0.0519\n",
            "Step 20 | loss = -0.0317 | ema = -0.0499\n",
            "Step 21 | loss = -0.0321 | ema = -0.0481\n",
            "Step 22 | loss = -0.0618 | ema = -0.0495\n",
            "Step 23 | loss = -0.0419 | ema = -0.0487\n",
            "Step 24 | loss = -0.0253 | ema = -0.0464\n",
            "Step 25 | loss = -0.0442 | ema = -0.0462\n",
            "Step 26 | loss = -0.0611 | ema = -0.0477\n",
            "Step 27 | loss = -0.0190 | ema = -0.0448\n",
            "Step 28 | loss = -0.0936 | ema = -0.0497\n",
            "Step 29 | loss = -0.0136 | ema = -0.0461\n",
            "Step 30 | loss = -0.0543 | ema = -0.0469\n",
            "Step 31 | loss = -0.0480 | ema = -0.0470\n",
            "Step 32 | loss = -0.0723 | ema = -0.0495\n",
            "Step 33 | loss = -0.0384 | ema = -0.0484\n",
            "Step 34 | loss = -0.0274 | ema = -0.0463\n",
            "Step 35 | loss = -0.0442 | ema = -0.0461\n",
            "Step 36 | loss = -0.0281 | ema = -0.0443\n",
            "Step 37 | loss = -0.0726 | ema = -0.0471\n",
            "Step 38 | loss = -0.0425 | ema = -0.0467\n",
            "Step 39 | loss = -0.0520 | ema = -0.0472\n",
            "Step 40 | loss = -0.0562 | ema = -0.0481\n",
            "Step 41 | loss = -0.0786 | ema = -0.0512\n",
            "Step 42 | loss = -0.0444 | ema = -0.0505\n",
            "Step 43 | loss = -0.0878 | ema = -0.0542\n",
            "Step 44 | loss = -0.0796 | ema = -0.0568\n",
            "Step 45 | loss = -0.0810 | ema = -0.0592\n",
            "Step 46 | loss = -0.0569 | ema = -0.0589\n",
            "Step 47 | loss = -0.0527 | ema = -0.0583\n",
            "Step 48 | loss = -0.0361 | ema = -0.0561\n",
            "Step 49 | loss = -0.0655 | ema = -0.0570\n",
            "\n",
            "Training kernel with 6 qubits\n",
            "Step 00 | loss = -0.0644 | ema = -0.0644\n",
            "Step 01 | loss = -0.0818 | ema = -0.0661\n",
            "Step 02 | loss = -0.0690 | ema = -0.0664\n",
            "Step 03 | loss = -0.0371 | ema = -0.0635\n",
            "Step 04 | loss = -0.0581 | ema = -0.0630\n",
            "Step 05 | loss = -0.0557 | ema = -0.0622\n",
            "Step 06 | loss = -0.0738 | ema = -0.0634\n",
            "Step 07 | loss = -0.0806 | ema = -0.0651\n",
            "Step 08 | loss = -0.0503 | ema = -0.0636\n",
            "Step 09 | loss = -0.0519 | ema = -0.0625\n",
            "Step 10 | loss = -0.0465 | ema = -0.0609\n",
            "Step 11 | loss = -0.0394 | ema = -0.0587\n",
            "Step 12 | loss = -0.0372 | ema = -0.0566\n",
            "Step 13 | loss = -0.0364 | ema = -0.0546\n",
            "Step 14 | loss = -0.0458 | ema = -0.0537\n",
            "Step 15 | loss = -0.0495 | ema = -0.0533\n",
            "Step 16 | loss = -0.0250 | ema = -0.0504\n",
            "Step 17 | loss = -0.0639 | ema = -0.0518\n",
            "Step 18 | loss = -0.0420 | ema = -0.0508\n",
            "Step 19 | loss = -0.0787 | ema = -0.0536\n",
            "Step 20 | loss = -0.0439 | ema = -0.0526\n",
            "Step 21 | loss = -0.0689 | ema = -0.0543\n",
            "Step 22 | loss = -0.0380 | ema = -0.0526\n",
            "Step 23 | loss = -0.0396 | ema = -0.0513\n",
            "Step 24 | loss = -0.0556 | ema = -0.0518\n",
            "Step 25 | loss = -0.0443 | ema = -0.0510\n",
            "Step 26 | loss = -0.0444 | ema = -0.0504\n",
            "Step 27 | loss = -0.0532 | ema = -0.0506\n",
            "Step 28 | loss = -0.0902 | ema = -0.0546\n",
            "Step 29 | loss = -0.0556 | ema = -0.0547\n",
            "Step 30 | loss = -0.0574 | ema = -0.0550\n",
            "Step 31 | loss = -0.0446 | ema = -0.0539\n",
            "Step 32 | loss = -0.1062 | ema = -0.0592\n",
            "Step 33 | loss = -0.0206 | ema = -0.0553\n",
            "Step 34 | loss = -0.0434 | ema = -0.0541\n",
            "Step 35 | loss = -0.0663 | ema = -0.0553\n",
            "Step 36 | loss = -0.0428 | ema = -0.0541\n",
            "Step 37 | loss = -0.0712 | ema = -0.0558\n",
            "Step 38 | loss = -0.0508 | ema = -0.0553\n",
            "Step 39 | loss = -0.0212 | ema = -0.0519\n",
            "Step 40 | loss = -0.0548 | ema = -0.0522\n",
            "Step 41 | loss = -0.0448 | ema = -0.0514\n",
            "Step 42 | loss = -0.0872 | ema = -0.0550\n",
            "Step 43 | loss = -0.0133 | ema = -0.0508\n",
            "Step 44 | loss = -0.0780 | ema = -0.0536\n",
            "Step 45 | loss = -0.0823 | ema = -0.0564\n",
            "Step 46 | loss = -0.0671 | ema = -0.0575\n",
            "Step 47 | loss = -0.0454 | ema = -0.0563\n",
            "Step 48 | loss = -0.0542 | ema = -0.0561\n",
            "Step 49 | loss = -0.0543 | ema = -0.0559\n",
            "\n",
            "Training kernel with 7 qubits\n",
            "Step 00 | loss = -0.0334 | ema = -0.0334\n",
            "Step 01 | loss = 0.0000 | ema = -0.0301\n",
            "Step 02 | loss = -0.0689 | ema = -0.0339\n",
            "Step 03 | loss = -0.0521 | ema = -0.0358\n",
            "Step 04 | loss = -0.0774 | ema = -0.0399\n",
            "Step 05 | loss = -0.0872 | ema = -0.0446\n",
            "Step 06 | loss = -0.0913 | ema = -0.0493\n",
            "Step 07 | loss = -0.0539 | ema = -0.0498\n",
            "Step 08 | loss = -0.0416 | ema = -0.0490\n",
            "Step 09 | loss = -0.0454 | ema = -0.0486\n",
            "Step 10 | loss = -0.0460 | ema = -0.0483\n",
            "Step 11 | loss = -0.0490 | ema = -0.0484\n",
            "Step 12 | loss = -0.0516 | ema = -0.0487\n",
            "Step 13 | loss = -0.0450 | ema = -0.0483\n",
            "Step 14 | loss = -0.0382 | ema = -0.0473\n",
            "Step 15 | loss = -0.0717 | ema = -0.0498\n",
            "Step 16 | loss = -0.0823 | ema = -0.0530\n",
            "Step 17 | loss = -0.0580 | ema = -0.0535\n",
            "Step 18 | loss = -0.0438 | ema = -0.0525\n",
            "Step 19 | loss = -0.0441 | ema = -0.0517\n",
            "Step 20 | loss = -0.0761 | ema = -0.0541\n",
            "Step 21 | loss = -0.0767 | ema = -0.0564\n",
            "Step 22 | loss = -0.0949 | ema = -0.0603\n",
            "Step 23 | loss = -0.1080 | ema = -0.0650\n",
            "Step 24 | loss = -0.0468 | ema = -0.0632\n",
            "Step 25 | loss = -0.0806 | ema = -0.0649\n",
            "Step 26 | loss = -0.0747 | ema = -0.0659\n",
            "Step 27 | loss = -0.0735 | ema = -0.0667\n",
            "Step 28 | loss = -0.0300 | ema = -0.0630\n",
            "Step 29 | loss = -0.0661 | ema = -0.0633\n",
            "Step 30 | loss = -0.0865 | ema = -0.0656\n",
            "Step 31 | loss = -0.0637 | ema = -0.0655\n",
            "Step 32 | loss = -0.0796 | ema = -0.0669\n",
            "Step 33 | loss = -0.0628 | ema = -0.0665\n",
            "Step 34 | loss = -0.0527 | ema = -0.0651\n",
            "Step 35 | loss = -0.0801 | ema = -0.0666\n",
            "Step 36 | loss = -0.0653 | ema = -0.0665\n",
            "Step 37 | loss = -0.0702 | ema = -0.0668\n",
            "Step 38 | loss = -0.0571 | ema = -0.0659\n",
            "Step 39 | loss = -0.0772 | ema = -0.0670\n",
            "Step 40 | loss = -0.0504 | ema = -0.0653\n",
            "Step 41 | loss = -0.0414 | ema = -0.0629\n",
            "Step 42 | loss = -0.0852 | ema = -0.0652\n",
            "Step 43 | loss = -0.0430 | ema = -0.0630\n",
            "Step 44 | loss = -0.0787 | ema = -0.0645\n",
            "Step 45 | loss = -0.0641 | ema = -0.0645\n",
            "Step 46 | loss = -0.0661 | ema = -0.0646\n",
            "Step 47 | loss = -0.0481 | ema = -0.0630\n",
            "Step 48 | loss = -0.0559 | ema = -0.0623\n",
            "Step 49 | loss = -0.0751 | ema = -0.0636\n",
            "\n",
            "Training kernel with 8 qubits\n",
            "Step 00 | loss = -0.0727 | ema = -0.0727\n",
            "Step 01 | loss = -0.0791 | ema = -0.0733\n",
            "Step 02 | loss = -0.0499 | ema = -0.0710\n",
            "Step 03 | loss = -0.0455 | ema = -0.0684\n",
            "Step 04 | loss = -0.0487 | ema = -0.0664\n",
            "Step 05 | loss = -0.0522 | ema = -0.0650\n",
            "Step 06 | loss = -0.0000 | ema = -0.0585\n",
            "Step 07 | loss = -0.0569 | ema = -0.0583\n",
            "Step 08 | loss = -0.0498 | ema = -0.0575\n",
            "Step 09 | loss = -0.0376 | ema = -0.0555\n",
            "Step 10 | loss = -0.0734 | ema = -0.0573\n",
            "Step 11 | loss = -0.0615 | ema = -0.0577\n",
            "Step 12 | loss = -0.0616 | ema = -0.0581\n",
            "Step 13 | loss = -0.0444 | ema = -0.0567\n",
            "Step 14 | loss = -0.0828 | ema = -0.0593\n",
            "Step 15 | loss = -0.0600 | ema = -0.0594\n",
            "Step 16 | loss = -0.0648 | ema = -0.0599\n",
            "Step 17 | loss = -0.0374 | ema = -0.0577\n",
            "Step 18 | loss = -0.0252 | ema = -0.0544\n",
            "Step 19 | loss = -0.0451 | ema = -0.0535\n",
            "Step 20 | loss = -0.0585 | ema = -0.0540\n",
            "Step 21 | loss = -0.0346 | ema = -0.0521\n",
            "Step 22 | loss = -0.0658 | ema = -0.0534\n",
            "Step 23 | loss = -0.0662 | ema = -0.0547\n",
            "Step 24 | loss = -0.0174 | ema = -0.0510\n",
            "Step 25 | loss = -0.0510 | ema = -0.0510\n",
            "Step 26 | loss = -0.0410 | ema = -0.0500\n",
            "Step 27 | loss = -0.0745 | ema = -0.0524\n",
            "Step 28 | loss = -0.0289 | ema = -0.0501\n",
            "Step 29 | loss = -0.0616 | ema = -0.0512\n",
            "Step 30 | loss = -0.0587 | ema = -0.0520\n",
            "Step 31 | loss = -0.0839 | ema = -0.0552\n",
            "Step 32 | loss = -0.0465 | ema = -0.0543\n",
            "Step 33 | loss = -0.0792 | ema = -0.0568\n",
            "Step 34 | loss = -0.0454 | ema = -0.0556\n",
            "Step 35 | loss = -0.0552 | ema = -0.0556\n",
            "Step 36 | loss = -0.0500 | ema = -0.0550\n",
            "Step 37 | loss = -0.0825 | ema = -0.0578\n",
            "Step 38 | loss = -0.0858 | ema = -0.0606\n",
            "Step 39 | loss = -0.0499 | ema = -0.0595\n",
            "Step 40 | loss = -0.0522 | ema = -0.0588\n",
            "Step 41 | loss = -0.0653 | ema = -0.0594\n",
            "Step 42 | loss = -0.0670 | ema = -0.0602\n",
            "Step 43 | loss = -0.0526 | ema = -0.0594\n",
            "Step 44 | loss = -0.0588 | ema = -0.0594\n",
            "Step 45 | loss = -0.0441 | ema = -0.0578\n",
            "Step 46 | loss = -0.0483 | ema = -0.0569\n",
            "Step 47 | loss = -0.0624 | ema = -0.0574\n",
            "Step 48 | loss = -0.0917 | ema = -0.0609\n",
            "Step 49 | loss = -0.0525 | ema = -0.0600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yFXHD8Mq--g"
      },
      "source": [
        "### Step 3: QJPEG Compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7NmV1jvNoXOs"
      },
      "outputs": [],
      "source": [
        "def vectorization(img, Cr, Cc, renorm=False):\n",
        "    \"Vectorize the image into amplitude-encoding patches suitable for quantum circuits\"\n",
        "    # splitting the original image (Mr, Mc) into S equal-size patches of shape (Cr, Cc)\n",
        "    Mr, Mc = img.shape\n",
        "    assert Mr % Cr == 0 and Mc % Cc == 0\n",
        "    patches = (img.reshape(Mc//Cr, Cr, -1, Cc).swapaxes(1, 2).reshape(-1, Cr, Cc))\n",
        "    # 64 patches, (64, 64, 64) shape; S=64\n",
        "\n",
        "    # vectorize each patch and collect all in a (N, Cr*Cc) array\n",
        "    vect_patches = np.reshape(patches,  (patches.shape[0], Cr*Cc)) # (64, 4096)\n",
        "\n",
        "    # normalize each (Cr*Cc) vector to the intensity of the corresponding (Cr, Cc) patch\n",
        "    states = np.zeros((patches.shape[0], Cr*Cc)) # (64, 4096)\n",
        "    norm = np.zeros(patches.shape[0])\n",
        "\n",
        "    for idx in range(patches.shape[0]): # for each patch\n",
        "        # compute the sum of pixels intensities\n",
        "        norm[idx] = vect_patches[idx].sum()\n",
        "        if norm[idx] == 0:\n",
        "            # empty patch -> encode |0...0>\n",
        "            states[idx, 0] = 1.0\n",
        "            norm[idx] = 1.0\n",
        "            continue\n",
        "\n",
        "        # normalize the patch vector so that its entries sum is 1\n",
        "        tmp = vect_patches[idx] / norm[idx]\n",
        "        # take the element-wise square root of the normalized vector\n",
        "        states[idx] = np.sqrt(tmp)\n",
        "    if renorm == False:\n",
        "        norm = np.ones(patches.shape[0])\n",
        "    print(states[:10])\n",
        "\n",
        "    return states, norm # amplitudes, pixel intensities' sums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cw7qW3kDrB8g"
      },
      "outputs": [],
      "source": [
        "def qft_swaps(wires):\n",
        "    n = len(wires)\n",
        "    # apply QFT to all qubits\n",
        "    qml.QFT(wires=wires)\n",
        "    # add swaps to reverse qubit order!\n",
        "    for i in range(n // 2):\n",
        "        qml.SWAP(wires=[wires[i], wires[n - i - 1]])\n",
        "\n",
        "\n",
        "def iqft_swaps(wires):\n",
        "    n = len(wires)\n",
        "    # swaps again - BEFORE iqft\n",
        "    for i in reversed(range(n // 2)):\n",
        "        qml.SWAP(wires=[wires[i], wires[n-i-1]])\n",
        "    qml.adjoint(QFT)(wires=wires)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "usd8_SMXyc_Y"
      },
      "outputs": [],
      "source": [
        "def circuit_builder(states, n0, n2, shots):\n",
        "    ntilde = (n0 - n2) // 2\n",
        "    n1 = n0 - ntilde\n",
        "\n",
        "    qnodes = []\n",
        "\n",
        "    # define device with n0 qubits\n",
        "    dev = qml.device(device, wires=n0, shots=shots)\n",
        "\n",
        "    for idx in range(states.shape[0]):\n",
        "        # qnode to capture current input state\n",
        "        @qml.qnode(dev)\n",
        "        def circuit():\n",
        "            # print(\"State norm:\", np.linalg.norm(states[idx]))\n",
        "            # initializing the state (using AmplitudeEmbedding here, but I'm wondering if something else could work faster)\n",
        "            qml.AmplitudeEmbedding(states[idx], wires=range(n0), pad_with=0.0, normalize=True)\n",
        "\n",
        "            # Hadamard on all n0 qubits\n",
        "            for w in range(n0):\n",
        "                qml.Hadamard(wires=w)\n",
        "\n",
        "            # apply QFT on all qubits\n",
        "            qft_swaps(wires=range(n0))\n",
        "\n",
        "            # apply IQFT on first n1 qubits\n",
        "            iqft_swaps(wires=range(n1))\n",
        "\n",
        "            # setting boundaries - Rule 2\n",
        "            discard_start = n0 // 2 - ntilde\n",
        "            discard_end = n0 // 2 - 1\n",
        "            discarded_qubits = set(range(discard_start, discard_end + 1))\n",
        "\n",
        "            # keep exactly n2 qubits for output\n",
        "            measured_qubits = list(range(n2))\n",
        "\n",
        "\n",
        "            # Hadamard on remaining qubits\n",
        "            for q in measured_qubits:\n",
        "                qml.Hadamard(wires=q)\n",
        "\n",
        "            # print(f'Measured qubits: {measured_qubits}')\n",
        "\n",
        "            return qml.probs(wires=measured_qubits)\n",
        "        qnodes.append(circuit)\n",
        "\n",
        "    return qnodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f8Cb9jBbodtD"
      },
      "outputs": [],
      "source": [
        "def reconstruction(qnodes, n2, norm):\n",
        "    out_freq = np.zeros((len(qnodes), 2**n2))\n",
        "    for idx, qnode in enumerate(qnodes):\n",
        "        probs = qnode()\n",
        "        out_freq[idx] = qnode() * norm[idx]\n",
        "\n",
        "    return out_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RLE8xtDaofNH"
      },
      "outputs": [],
      "source": [
        "def devectorization(out_freq):\n",
        "    S = out_freq.shape[0]\n",
        "    nrow = int(np.sqrt(out_freq.shape[1])) # rows per patch\n",
        "    ncol = nrow\n",
        "\n",
        "    decoded_patches = np.reshape(out_freq,\\\n",
        "                      (out_freq.shape[0], nrow, ncol)) # (S, nrow, ncol)\n",
        "\n",
        "    im_h, im_w = nrow*int(np.sqrt(S)), ncol*int(np.sqrt(S)) # final shape\n",
        "\n",
        "    # initialization\n",
        "    decoded_img = np.zeros((im_w, im_h))\n",
        "\n",
        "    idx = 0\n",
        "    for row in np.arange(im_h - nrow + 1, step=nrow):\n",
        "        for col in np.arange(im_w - ncol + 1, step=ncol):\n",
        "            decoded_img[row:row+nrow, col:col+ncol] = decoded_patches[idx]\n",
        "            idx += 1\n",
        "\n",
        "    return decoded_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nfA2v4IoTpE0"
      },
      "outputs": [],
      "source": [
        "def qjpeg_feature_map_quantum(img_28x28):\n",
        "    \"\"\"\n",
        "    True QJPEG-inspired feature map:\n",
        "    - probabilities sum to 1\n",
        "    - amplitudes = sqrt(probabilities)\n",
        "    - output dimension = 64 (6 qubits)\n",
        "    \"\"\"\n",
        "\n",
        "    img = img_28x28.astype(float)\n",
        "    img = img / img.sum()              # probabilities\n",
        "    amps = np.sqrt(img.flatten())      # amplitudes\n",
        "\n",
        "    # reduce to 64 amplitudes (simple truncation for now)\n",
        "    amps = amps[:64]\n",
        "\n",
        "    # safety\n",
        "    if np.linalg.norm(amps) == 0:\n",
        "        amps[0] = 1.0\n",
        "    else:\n",
        "        amps /= np.linalg.norm(amps)\n",
        "\n",
        "    return amps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J-NBBmzuDeb"
      },
      "source": [
        "### Step 4: Inference without retraining"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C = 100.0  # SVM regularization - the higher, the better\n",
        "n_qubits_list = [4, 5, 6, 7, 8]\n",
        "n_layers = 2\n",
        "alpha = 0.8 # weight for combining QEK + QJPEG"
      ],
      "metadata": {
        "id": "HMAyxZpuKLbo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_and_normalize(x, n_qubits, eps=1e-12):\n",
        "    \"\"\"Truncate vector to 2**n_qubits and normalize for QEK input.\"\"\"\n",
        "    dim = 2 ** n_qubits\n",
        "    v = x[:dim].copy()\n",
        "    norm = np.linalg.norm(v)\n",
        "    if norm < eps:\n",
        "        v[0] = 1.0\n",
        "        v[1:] = 0.0\n",
        "        norm = 1.0\n",
        "    return v / norm\n",
        "\n",
        "def qjpeg_to_32(img_28x28):\n",
        "    \"\"\"Return 32-dimensional QJPEG vector from 28x28 image.\"\"\"\n",
        "    amps = qjpeg_feature_map_quantum(img_28x28)  # get 64 amplitudes\n",
        "    amps_32 = amps[:32]  # truncate to 32\n",
        "    norm = np.linalg.norm(amps_32)\n",
        "    if norm == 0:\n",
        "        amps_32[0] = 1.0\n",
        "        norm = 1.0\n",
        "    return amps_32 / norm\n",
        "\n",
        "def prepare_qek_features(X, n_qubits):\n",
        "    return np.array([truncate_and_normalize(x, n_qubits) for x in X])\n",
        "\n",
        "def compute_kernel(states_a, states_b=None):\n",
        "    \"\"\"Compute kernel matrix from quantum states.\"\"\"\n",
        "    if states_b is None:\n",
        "        states_b = states_a\n",
        "    K = np.zeros((len(states_a), len(states_b)))\n",
        "    for i, a in enumerate(states_a):\n",
        "        for j, b in enumerate(states_b):\n",
        "            K[i, j] = np.abs(np.vdot(a, b))**2\n",
        "    return K\n",
        "# Inference loop\n",
        "results = []\n",
        "\n",
        "for n_qubits in n_qubits_list:\n",
        "    wires = range(n_qubits)\n",
        "    dev = qml.device(device, wires=wires, shots=None)\n",
        "\n",
        "    # Prepare features\n",
        "    X_train_qek_n = prepare_qek_features(X_train_qek, n_qubits)\n",
        "    X_test_qek_n  = prepare_qek_features(X_test_qek, n_qubits)\n",
        "\n",
        "    # Expressive QNode\n",
        "    @qml.qnode(dev, interface=\"autograd\")\n",
        "    def expressive_state(x, theta):\n",
        "        feature_map(x, theta, wires=wires)\n",
        "        return qml.state()\n",
        "\n",
        "    # Truncate theta if needed\n",
        "    n_layers_trained, n_qubits_trained, _ = theta.shape\n",
        "    theta_n = theta[:, :n_qubits, :].copy() if n_qubits <= n_qubits_trained else \\\n",
        "              np.concatenate([theta, 0.01*np.random.randn(n_layers_trained, n_qubits - n_qubits_trained, 3)], axis=1)\n",
        "\n",
        "    # Compute QEK states\n",
        "    states_train = np.array([expressive_state(x, theta_n) for x in X_train_qek_n])\n",
        "    states_test  = np.array([expressive_state(x, theta_n) for x in X_test_qek_n])\n",
        "\n",
        "    # Kernel\n",
        "    K_train = compute_kernel(states_train)\n",
        "    K_test  = compute_kernel(states_test, states_train)\n",
        "    K_train /= np.linalg.norm(K_train)\n",
        "    K_test  /= np.linalg.norm(K_train)\n",
        "\n",
        "    # SVM\n",
        "    clf = SVC(kernel=\"precomputed\", C=C)\n",
        "    clf.fit(K_train, y_train)\n",
        "    y_pred = clf.predict(K_test)\n",
        "    acc_qek = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store everything\n",
        "    results.append({\n",
        "        \"n_qubits\": n_qubits,\n",
        "        \"acc_qek\": acc_qek,\n",
        "        \"states_train\": states_train,\n",
        "        \"states_test\": states_test\n",
        "    })\n",
        "\n",
        "def combine_qek_qjpeg(qek_states, qjpeg_features, alpha=0.8):\n",
        "    combined = [np.concatenate([alpha * q, (1-alpha) * c]) for q, c in zip(qek_states, qjpeg_features)]\n",
        "    combined = [v / np.linalg.norm(v) for v in combined]\n",
        "    return np.array(combined)\n",
        "\n",
        "X_train_qjpeg_32 = np.array([qjpeg_to_32(img.reshape(28,28)) for img in X_train_img])\n",
        "X_test_qjpeg_32  = np.array([qjpeg_to_32(img.reshape(28,28)) for img in X_test_img])\n",
        "\n",
        "for r in results:\n",
        "    n_qubits = r[\"n_qubits\"]\n",
        "    states_train_n = r[\"states_train\"]\n",
        "    states_test_n  = r[\"states_test\"]\n",
        "\n",
        "    X_train_comb = combine_qek_qjpeg(states_train_n, X_train_qjpeg_32, alpha)\n",
        "    X_test_comb  = combine_qek_qjpeg(states_test_n, X_test_qjpeg_32, alpha)\n",
        "\n",
        "    K_train_comb = compute_kernel(X_train_comb)\n",
        "    K_test_comb  = compute_kernel(X_test_comb, X_train_comb)\n",
        "    K_train_comb /= np.linalg.norm(K_train_comb)\n",
        "    K_test_comb  /= np.linalg.norm(K_train_comb)\n",
        "\n",
        "    clf_comb = SVC(kernel=\"precomputed\", C=C)\n",
        "    clf_comb.fit(K_train_comb, y_train)\n",
        "    y_pred_comb = clf_comb.predict(K_test_comb)\n",
        "    acc_combined = accuracy_score(y_test, y_pred_comb)\n",
        "    r[\"acc_combined\"] = acc_combined\n",
        "\n",
        "for r in results:\n",
        "    print(f\"\\n{'='*50}\\nInference with {r[\"n_qubits\"]} qubits\\n{'='*50}\")\n",
        "    print(f\"QEK accuracy: {r[\"acc_qek\"]:.4f}\")\n",
        "    print(f\"QEK + QJPEG accuracy: {r[\"acc_combined\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0BdoJqsEXiy",
        "outputId": "3febf69a-5818-43ae-8f21-c403ade0c0ae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Inference with 4 qubits\n",
            "==================================================\n",
            "QEK accuracy: 0.9500\n",
            "QEK + QJPEG accuracy: 0.9500\n",
            "\n",
            "==================================================\n",
            "Inference with 5 qubits\n",
            "==================================================\n",
            "QEK accuracy: 0.5500\n",
            "QEK + QJPEG accuracy: 0.5500\n",
            "\n",
            "==================================================\n",
            "Inference with 6 qubits\n",
            "==================================================\n",
            "QEK accuracy: 0.8500\n",
            "QEK + QJPEG accuracy: 0.8000\n",
            "\n",
            "==================================================\n",
            "Inference with 7 qubits\n",
            "==================================================\n",
            "QEK accuracy: 0.8500\n",
            "QEK + QJPEG accuracy: 0.8000\n",
            "\n",
            "==================================================\n",
            "Inference with 8 qubits\n",
            "==================================================\n",
            "QEK accuracy: 0.8500\n",
            "QEK + QJPEG accuracy: 0.8000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KROFpzJsoxfN"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}